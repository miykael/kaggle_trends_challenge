{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import nilearn as nl\n",
    "from nilearn import image, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brain mask\n",
    "mask = nl.image.load_img('templates/fMRI_mask.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GM, WM and CSF probability maps from ICBM 2009c asymmetric template\n",
    "# From: http://www.bic.mni.mcgill.ca/ServicesAtlases/ICBM152NLin2009\n",
    "template_gm = nl.image.load_img('templates/mni_icbm152_nlin_asym_09c/mni_icbm152_gm_tal_nlin_asym_09c.nii')\n",
    "template_gm = image.resample_to_img(template_gm, mask)\n",
    "pve_gm = image.math_img('img * mask', img=template_gm, mask=mask)\n",
    "\n",
    "template_wm = nl.image.load_img('templates/mni_icbm152_nlin_asym_09c/mni_icbm152_wm_tal_nlin_asym_09c.nii')\n",
    "template_wm = image.resample_to_img(template_wm, mask)\n",
    "pve_wm = image.math_img('img * mask', img=template_wm, mask=mask)\n",
    "\n",
    "template_csf = nl.image.load_img('templates/mni_icbm152_nlin_asym_09c/mni_icbm152_csf_tal_nlin_asym_09c.nii')\n",
    "template_csf = image.resample_to_img(template_csf, mask)\n",
    "pve_csf = image.math_img('img * mask', img=template_csf, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pve mask\n",
    "pve_concat = image.concat_imgs([pve_gm, pve_wm, pve_csf])\n",
    "pve_mask = image.math_img('np.sum(img, axis=-1)>0.5', img=pve_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pve binary masks per tissue\n",
    "pve_argmax = image.math_img('np.argmax(img, axis=-1) * mask', img=pve_concat, mask=mask)\n",
    "pve_mask_gm, pve_mask_wm, pve_mask_csf = [image.math_img('(img==%d)*mask' % i, img=pve_argmax, mask=mask) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(filename, mask):\n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        data = np.array(f['SM_feature'], dtype='float32')\n",
    "\n",
    "    # It's necessary to reorient the axes, since h5py flips axis order\n",
    "    data = np.moveaxis(data, [0, 1, 2, 3],\n",
    "                             [3, 2, 1, 0])\n",
    "\n",
    "    img = nl.image.new_img_like(mask, data, affine=mask.affine, copy_header=True)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Rewrite mat file to compressed NIfTI\n",
    "    for fname in tqdm(sorted(glob('fMRI_tr*/*.mat'))):\n",
    "        new_filename = fname.replace('.mat', '.nii.gz')\n",
    "        read_img(fname, mask).to_filename(new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    # Rewrite mat file to compressed NIfTI\n",
    "    for fname in tqdm(sorted(glob('fMRI_te*/*.mat'))):\n",
    "        new_filename = fname.replace('.mat', '.nii.gz')\n",
    "        read_img(fname, mask).to_filename(new_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explor NIfTI files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image, plotting, masking\n",
    "from nilearn.regions import connected_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(glob('fMRI_train/*.nii.gz'))[::10]\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the mean image for a given component\n",
    "def get_mean_component(filenames, comp_ID=0):\n",
    "    mean = image.math_img('img * 0', img=mask)\n",
    "    for f in tqdm(filenames):\n",
    "        img = image.load_img(f).slicer[..., comp_ID]\n",
    "        mean = image.math_img('mean + img', mean=mean, img=img)\n",
    "    mean = image.math_img('img / %f' % len(filenames), img=mean)\n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the mean images\n",
    "for idx in range(53):\n",
    "    mean = get_mean_component(filenames, comp_ID=idx)\n",
    "    mean.to_filename('fMRI_maps/mean_%02d.nii.gz' % (idx + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the mean images\n",
    "for idx in range(5):\n",
    "    img = image.load_img('fMRI_maps/mean_%02d.nii.gz' % (idx + 1))\n",
    "    data = img.get_fdata()\n",
    "    threshold = np.percentile(data[data!=0], 99)\n",
    "    img_thr = image.threshold_img(img, threshold=threshold)\n",
    "    img_regions = image.mean_img(connected_regions(img_thr, min_region_size=4000)[0])\n",
    "    plotting.plot_glass_brain(img_regions, black_bg=True, display_mode='lyrz',\n",
    "                              title='mean_%02d' % (idx + 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant features from regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls datasets/*raw_train_*csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(glob('datasets/*raw_train_*csv'))\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect value metrics from images\n",
    "train_files = sorted(glob('fMRI_train/*.nii.gz'))\n",
    "\n",
    "for idx in range(n_train, 53):\n",
    "\n",
    "    # Load mean image\n",
    "    mean = image.load_img('fMRI_maps/mean_%02d.nii.gz' % (idx + 1))\n",
    "    data_mean = mean.get_fdata()[mask.get_fdata()>0]\n",
    "    \n",
    "    # Compute binary mask for region\n",
    "    mask_region = data_mean > np.percentile(data_mean, 99)\n",
    "\n",
    "    # Store results in results file\n",
    "    results = {}\n",
    "    \n",
    "    for t in tqdm(train_files):\n",
    "\n",
    "        try:\n",
    "            # Get file name\n",
    "            t_id = t.split('/')[1].split('.')[0]\n",
    "\n",
    "            # Load current volume\n",
    "            img = image.index_img(t, idx)\n",
    "\n",
    "            # Only extract data values from within mask\n",
    "            data_img = img.get_fdata()[mask.get_fdata()>0]\n",
    "\n",
    "            # Collect correlation coefficient to mean image\n",
    "            corr_coef = np.corrcoef(data_img, data_mean)[0, 1]\n",
    "\n",
    "            # Collect euclidean distance to mean image\n",
    "            euclide_whole = np.linalg.norm(np.subtract(data_img, data_mean))\n",
    "            euclide_region = np.linalg.norm(np.subtract(data_img[mask_region], data_mean[mask_region]))\n",
    "\n",
    "            # Collect percentiles from whole image and region\n",
    "            perc_to_check_r = [0.1, 1, 5, 50, 95, 99, 99.9]\n",
    "            percentiles_whole = [np.percentile(data_img, p) for p in perc_to_check_r]\n",
    "            percentiles_region = [np.percentile(data_img[mask_region], p) for p in perc_to_check_r]\n",
    "\n",
    "            # Extract volume data from within PVE masks\n",
    "            pve_masked_gm_values = img.get_fdata()[pve_mask_gm.get_fdata().astype('bool')]\n",
    "            pve_masked_wm_values = img.get_fdata()[pve_mask_wm.get_fdata().astype('bool')]\n",
    "            pve_masked_csf_values = img.get_fdata()[pve_mask_csf.get_fdata().astype('bool')]\n",
    "\n",
    "            # Collect percentiles from tissue masks\n",
    "            perc_to_check_t = [1, 5, 50, 95, 99]\n",
    "            percentiles_gm = [np.percentile(pve_masked_gm_values, p) for p in perc_to_check_t]\n",
    "            percentiles_wm = [np.percentile(pve_masked_wm_values, p) for p in perc_to_check_t]\n",
    "            percentiles_csf = [np.percentile(pve_masked_csf_values, p) for p in perc_to_check_t]\n",
    "\n",
    "            # Compute smoothness to original image difference\n",
    "            smoothness = np.linalg.norm(image.math_img(\n",
    "                'img-smooth', img=img, smooth=image.smooth_img(\n",
    "                    img, 6)).get_fdata()[mask.get_fdata()>0])\n",
    "\n",
    "            # Compute coefficient of joint variation (CJV) within GM and WM\n",
    "            cjv = (pve_masked_wm_values.std() + pve_masked_gm_values.std()) / \\\n",
    "                   np.abs(pve_masked_wm_values.mean() - pve_masked_gm_values.mean())\n",
    "\n",
    "            # Compute signal to noise ratio\n",
    "            snr_gm = pve_masked_gm_values.mean() / (pve_masked_gm_values.std() * np.sqrt(len(pve_masked_gm_values)/(len(pve_masked_gm_values)-1)))\n",
    "            snr_wm = pve_masked_wm_values.mean() / (pve_masked_wm_values.std() * np.sqrt(len(pve_masked_wm_values)/(len(pve_masked_wm_values)-1)))\n",
    "            snr_csf = pve_masked_csf_values.mean() / (pve_masked_csf_values.std() * np.sqrt(len(pve_masked_csf_values)/(len(pve_masked_csf_values)-1)))\n",
    "\n",
    "            # Compute wm2max values\n",
    "            muWM = pve_masked_wm_values.mean()\n",
    "            wm2max_gm = muWM/np.percentile(pve_masked_gm_values, 99.95)\n",
    "            wm2max_wm = muWM/np.percentile(pve_masked_wm_values, 99.95)\n",
    "            wm2max_csf = muWM/np.percentile(pve_masked_csf_values, 99.95)\n",
    "\n",
    "            # Collect standard deviation from whole image and region\n",
    "            whole_std = data_img.std()\n",
    "            region_std = data_img[mask_region].std()\n",
    "\n",
    "            results[t_id] = [\n",
    "                t_id, corr_coef, euclide_whole, euclide_region, whole_std, region_std,\n",
    "                *percentiles_whole, *percentiles_region,\n",
    "                *percentiles_gm, *percentiles_wm, *percentiles_csf,\n",
    "                smoothness, cjv,\n",
    "                snr_gm, snr_wm, snr_csf,\n",
    "                wm2max_gm, wm2max_wm, wm2max_csf,\n",
    "                ]\n",
    "        except:\n",
    "            print(t)\n",
    "\n",
    "    # Store result in CSV file\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    df_results.columns = ['Id', 'corr_coef',\n",
    "                          'euclide_whole', 'euclide_region',\n",
    "                          'whole_std', 'region_std',\n",
    "                          *['perc-%s_whole' % p for p in perc_to_check_r],\n",
    "                          *['perc-%s_region' % p for p in perc_to_check_r],\n",
    "                          *['perc-%s_gm' % p for p in perc_to_check_t],\n",
    "                          *['perc-%s_wm' % p for p in perc_to_check_t],\n",
    "                          *['perc-%s_csf' % p for p in perc_to_check_t],\n",
    "                          'smoothness',\n",
    "                          'cjv',\n",
    "                          'snr_gm', 'snr_wm', 'snr_csf',\n",
    "                          'wm2max_gm', 'wm2max_wm', 'wm2max_csf']\n",
    "    df_results.to_csv('datasets/brain_values_raw_train_%02d.csv' % (idx + 1), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls datasets/*values_raw_test*csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = len(glob('datasets/*raw_test_*csv'))\n",
    "n_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Collect results\n",
    "results = {}\n",
    "\n",
    "test_files = sorted(glob('fMRI_test/*.nii.gz'))\n",
    "\n",
    "for idx in range(n_test, 53):\n",
    "\n",
    "    # Load mean image\n",
    "    mean = image.load_img('fMRI_maps/mean_%02d.nii.gz' % (idx + 1))\n",
    "    data_mean = mean.get_fdata()[mask.get_fdata()>0]\n",
    "    \n",
    "    # Compute binary mask for region\n",
    "    mask_region = data_mean > np.percentile(data_mean, 99)\n",
    "\n",
    "    # Store results in results file\n",
    "    results = {}\n",
    "    \n",
    "    for t in tqdm(test_files):\n",
    "\n",
    "        try:\n",
    "            # Get file name\n",
    "            t_id = t.split('/')[1].split('.')[0]\n",
    "\n",
    "            # Load current volume\n",
    "            img = image.index_img(t, idx)\n",
    "\n",
    "            # Only extract data values from within mask\n",
    "            data_img = img.get_fdata()[mask.get_fdata()>0]\n",
    "\n",
    "            # Collect correlation coefficient to mean image\n",
    "            corr_coef = np.corrcoef(data_img, data_mean)[0, 1]\n",
    "\n",
    "            # Collect euclidean distance to mean image\n",
    "            euclide_whole = np.linalg.norm(np.subtract(data_img, data_mean))\n",
    "            euclide_region = np.linalg.norm(np.subtract(data_img[mask_region], data_mean[mask_region]))\n",
    "\n",
    "            # Collect percentiles from whole image and region\n",
    "            perc_to_check_r = [0.1, 1, 5, 50, 95, 99, 99.9]\n",
    "            percentiles_whole = [np.percentile(data_img, p) for p in perc_to_check_r]\n",
    "            percentiles_region = [np.percentile(data_img[mask_region], p) for p in perc_to_check_r]\n",
    "\n",
    "            # Extract volume data from within PVE masks\n",
    "            pve_masked_gm_values = img.get_fdata()[pve_mask_gm.get_fdata().astype('bool')]\n",
    "            pve_masked_wm_values = img.get_fdata()[pve_mask_wm.get_fdata().astype('bool')]\n",
    "            pve_masked_csf_values = img.get_fdata()[pve_mask_csf.get_fdata().astype('bool')]\n",
    "\n",
    "            # Collect percentiles from tissue masks\n",
    "            perc_to_check_t = [1, 5, 50, 95, 99]\n",
    "            percentiles_gm = [np.percentile(pve_masked_gm_values, p) for p in perc_to_check_t]\n",
    "            percentiles_wm = [np.percentile(pve_masked_wm_values, p) for p in perc_to_check_t]\n",
    "            percentiles_csf = [np.percentile(pve_masked_csf_values, p) for p in perc_to_check_t]\n",
    "\n",
    "            # Compute smoothness to original image difference\n",
    "            smoothness = np.linalg.norm(image.math_img(\n",
    "                'img-smooth', img=img, smooth=image.smooth_img(\n",
    "                    img, 6)).get_fdata()[mask.get_fdata()>0])\n",
    "\n",
    "            # Compute coefficient of joint variation (CJV) within GM and WM\n",
    "            cjv = (pve_masked_wm_values.std() + pve_masked_gm_values.std()) / \\\n",
    "                   np.abs(pve_masked_wm_values.mean() - pve_masked_gm_values.mean())\n",
    "\n",
    "            # Compute signal to noise ratio\n",
    "            snr_gm = pve_masked_gm_values.mean() / (pve_masked_gm_values.std() * np.sqrt(len(pve_masked_gm_values)/(len(pve_masked_gm_values)-1)))\n",
    "            snr_wm = pve_masked_wm_values.mean() / (pve_masked_wm_values.std() * np.sqrt(len(pve_masked_wm_values)/(len(pve_masked_wm_values)-1)))\n",
    "            snr_csf = pve_masked_csf_values.mean() / (pve_masked_csf_values.std() * np.sqrt(len(pve_masked_csf_values)/(len(pve_masked_csf_values)-1)))\n",
    "\n",
    "            # Compute wm2max values\n",
    "            muWM = pve_masked_wm_values.mean()\n",
    "            wm2max_gm = muWM/np.percentile(pve_masked_gm_values, 99.95)\n",
    "            wm2max_wm = muWM/np.percentile(pve_masked_wm_values, 99.95)\n",
    "            wm2max_csf = muWM/np.percentile(pve_masked_csf_values, 99.95)\n",
    "\n",
    "            # Collect standard deviation from whole image and region\n",
    "            whole_std = data_img.std()\n",
    "            region_std = data_img[mask_region].std()\n",
    "\n",
    "            results[t_id] = [\n",
    "                t_id, corr_coef, euclide_whole, euclide_region, whole_std, region_std,\n",
    "                *percentiles_whole, *percentiles_region,\n",
    "                *percentiles_gm, *percentiles_wm, *percentiles_csf,\n",
    "                smoothness, cjv,\n",
    "                snr_gm, snr_wm, snr_csf,\n",
    "                wm2max_gm, wm2max_wm, wm2max_csf,\n",
    "                ]\n",
    "        except:\n",
    "            print(t)\n",
    "\n",
    "    # Store result in CSV file\n",
    "    df_results = pd.DataFrame(results).T\n",
    "    df_results.columns = ['Id', 'corr_coef',\n",
    "                          'euclide_whole', 'euclide_region',\n",
    "                          'whole_std', 'region_std',\n",
    "                          *['perc-%s_whole' % p for p in perc_to_check_r],\n",
    "                          *['perc-%s_region' % p for p in perc_to_check_r],\n",
    "                          *['perc-%s_gm' % p for p in perc_to_check_t],\n",
    "                          *['perc-%s_wm' % p for p in perc_to_check_t],\n",
    "                          *['perc-%s_csf' % p for p in perc_to_check_t],\n",
    "                          'smoothness',\n",
    "                          'cjv',\n",
    "                          'snr_gm', 'snr_wm', 'snr_csf',\n",
    "                          'wm2max_gm', 'wm2max_wm', 'wm2max_csf']\n",
    "    df_results.to_csv('datasets/brain_values_raw_test_%02d.csv' % (idx + 1), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract relevant features within subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "corr_results = {}\n",
    "\n",
    "# Collect all train files\n",
    "train_files = sorted(glob('fMRI_train/*.nii.gz'))\n",
    "\n",
    "for t in tqdm(train_files):\n",
    "\n",
    "    try:\n",
    "        # Load mean image\n",
    "        img = image.load_img(t)\n",
    "        data = img.get_fdata()[mask.get_fdata()>0]\n",
    "\n",
    "        t_id = t.split('/')[1].split('.')[0]\n",
    "        corr_results[t_id] = np.ravel(np.corrcoef(data.T))\n",
    "\n",
    "    except:\n",
    "            print(t)\n",
    "\n",
    "df_corr = pd.DataFrame(corr_results).T\n",
    "df_corr.columns = ['c%02d_c%02d' % (i + 1, j + 1)\n",
    "                   for i in range(53) for j in range(53)]\n",
    "\n",
    "# Only keep upper triangular correlation matrix without diagonal\n",
    "triangular_mask = np.ravel(np.triu(np.ones((53, 53)), k=1))>0.5\n",
    "df_corr = df_corr.loc[:, triangular_mask]\n",
    "\n",
    "# Save everything in CSV file\n",
    "df_corr.to_csv('datasets/brain_corr_raw_train.csv')\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results\n",
    "corr_results = {}\n",
    "\n",
    "# Collect all test files\n",
    "test_files = sorted(glob('fMRI_test/*.nii.gz'))\n",
    "\n",
    "for t in tqdm(test_files):\n",
    "\n",
    "    try:\n",
    "        # Load mean image\n",
    "        img = image.load_img(t)\n",
    "        data = img.get_fdata()[mask.get_fdata()>0]\n",
    "\n",
    "        t_id = t.split('/')[1].split('.')[0]\n",
    "        corr_results[t_id] = np.ravel(np.corrcoef(data.T))\n",
    "\n",
    "    except:\n",
    "            print(t)\n",
    "\n",
    "df_corr = pd.DataFrame(corr_results).T\n",
    "df_corr.columns = ['c%02d_c%02d' % (i + 1, j + 1)\n",
    "                   for i in range(53) for j in range(53)]\n",
    "\n",
    "# Only keep upper triangular correlation matrix without diagonal\n",
    "triangular_mask = np.ravel(np.triu(np.ones((53, 53)), k=1))>0.5\n",
    "df_corr = df_corr.loc[:, triangular_mask]\n",
    "\n",
    "# Save everything in CSV file\n",
    "df_corr.to_csv('datasets/brain_corr_raw_test.csv')\n",
    "df_corr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find region correlating high with target categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls datasets/*brain_vox_raw_test*csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(glob('datasets/*brain_vox_raw_test_*csv'))\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load brain mask\n",
    "mask = nl.image.load_img('templates/fMRI_mask.nii')\n",
    "\n",
    "# Collect value metrics from images\n",
    "train_files = sorted(glob('fMRI_train/*.nii.gz'))\n",
    "test_files = sorted(glob('fMRI_test/*.nii.gz'))\n",
    "\n",
    "for idx in range(n_train, 53):\n",
    "    \n",
    "    data_collection = []\n",
    "    tid_collection = []\n",
    "\n",
    "    for t in tqdm(train_files):\n",
    "\n",
    "        try:\n",
    "            # Load current volume and extract data values only from within the mask\n",
    "            data_img = image.index_img(t, idx).get_fdata()[mask.get_fdata()>0]\n",
    "\n",
    "            # Store data\n",
    "            data_collection.append(data_img)\n",
    "\n",
    "            # Get file name\n",
    "            t_id = t.split('/')[1].split('.')[0]\n",
    "            tid_collection.append(t_id)\n",
    "\n",
    "        except:\n",
    "            print(t_id)\n",
    "\n",
    "    data_collection = np.array(data_collection)\n",
    "    tid_collection = np.array(tid_collection).astype('int')\n",
    "    \n",
    "    # To save all relevant voxel_idx\n",
    "    voxel_to_extract = []\n",
    "\n",
    "    # Select top correlating voxels in two different splits\n",
    "    nsteps = 2\n",
    "\n",
    "    target_column = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']\n",
    "    \n",
    "    for t in tqdm(target_column):\n",
    "\n",
    "        print('Computes the correlation coefficients for: %s' % (t))\n",
    "\n",
    "        # For every component, randomly reshuffle order of entries\n",
    "        target = pd.read_csv('train_scores.csv')[['Id', t]].dropna()\n",
    "\n",
    "        # Find overlap between target ID and brains\n",
    "        overlap = np.intersect1d(target.Id, tid_collection)\n",
    "        \n",
    "        # Keep only overlaps\n",
    "        target = target[target.Id.isin(overlap)]\n",
    "        data_target = data_collection[[t in overlap for t in tid_collection]]\n",
    "        tid_target = tid_collection[[t in overlap for t in tid_collection]]\n",
    "        \n",
    "        # Split data randomly in two parts\n",
    "        corr_set = train_test_split(target[t].values, data_target, tid_target,\n",
    "                                    train_size=0.5, shuffle=True)\n",
    "        \n",
    "        for i_start in range(2):\n",
    "\n",
    "            # Prepare datasets\n",
    "            target_set, data_set, tid_set = corr_set[i_start], corr_set[i_start+2], corr_set[i_start+4]\n",
    "            \n",
    "            # Compute correlation between data and target\n",
    "            data_corr = []\n",
    "            for v in range(data_set.shape[1]):\n",
    "                dfocus = data_set[:, v]\n",
    "                dfocus -= np.median(dfocus)\n",
    "                chigh, clow = np.percentile(dfocus[dfocus>0], 99), np.percentile(dfocus[dfocus<0], 1)\n",
    "                cselecter = np.logical_and(dfocus<chigh, dfocus>clow)\n",
    "\n",
    "                data_corr.append(np.corrcoef(dfocus[cselecter], target_set[cselecter])[0, 1])\n",
    "            data_corr = np.array(data_corr)\n",
    "            \n",
    "            # Find voxels with top target correlation\n",
    "            top_sort = np.argsort(np.abs(data_corr))[::-1][:10]\n",
    "\n",
    "            # Find voxel id of orthogonal (most uncorrelated voxel with top correlated voxel)\n",
    "            orth_corr_id = np.argmin(np.abs(np.corrcoef(data_collection[:, top_sort].T)[0, :]))\n",
    "\n",
    "            # Specify which voxels to look at\n",
    "            vox_select = [top_sort[0], top_sort[orth_corr_id]]\n",
    "\n",
    "            # Add top voxels to voxel_idx\n",
    "            voxel_to_extract.append(vox_select)\n",
    "\n",
    "    voxel_to_extract = np.unique(voxel_to_extract)\n",
    "    \n",
    "    # Save voxel info in CSV file\n",
    "    df_out = pd.DataFrame(data_collection[:, voxel_to_extract])\n",
    "    df_out.columns = ['vox_corr_idx%02d_c%02d' % (idx + 1, cix + 1) for cix in range(len(voxel_to_extract))]\n",
    "    df_out.insert(0, 'Id', tid_collection)\n",
    "    df_out.set_index('Id', inplace=True)\n",
    "    df_out.to_csv('datasets/brain_vox_raw_train_%02d.csv' % (idx + 1))\n",
    "    \n",
    "    # Collect same voxel information from test set\n",
    "    data_collection = []\n",
    "    tid_collection = []\n",
    "\n",
    "    for t in tqdm(test_files):\n",
    "\n",
    "        try:\n",
    "            # Load current volume and extract data values only from within the mask\n",
    "            data_img = image.index_img(t, idx).get_fdata()[mask.get_fdata()>0]\n",
    "\n",
    "            # Store data\n",
    "            data_collection.append(data_img)\n",
    "\n",
    "            # Get file name\n",
    "            t_id = t.split('/')[1].split('.')[0]\n",
    "            tid_collection.append(t_id)\n",
    "\n",
    "        except:\n",
    "            print(t_id)\n",
    "\n",
    "    data_collection = np.array(data_collection)\n",
    "    tid_collection = np.array(tid_collection).astype('int')\n",
    "\n",
    "    # Save voxel info in CSV file\n",
    "    df_out = pd.DataFrame(data_collection[:, voxel_to_extract])\n",
    "    df_out.columns = ['vox_corr_idx%02d_c%02d' % (idx + 1, cix + 1) for cix in range(len(voxel_to_extract))]\n",
    "    df_out.insert(0, 'Id', tid_collection)\n",
    "    df_out.set_index('Id', inplace=True)\n",
    "    df_out.to_csv('datasets/brain_vox_raw_test_%02d.csv' % (idx + 1))\n",
    "    \n",
    "    print('Component %02d finished.' % (idx + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
